Summary Table of Data
```{r}
# Summary of Data
trips <- read.csv("trips_per_day.csv")
```


```{r}
library(dplyr)
library(lubridate)

trips <- trips %>%
  mutate(trip_date = as.Date(trip_date))

summary_table <- trips %>%
  summarise(
    time_span_start = min(trip_date, na.rm = TRUE),
    time_span_end   = max(trip_date, na.rm = TRUE),
    total_days      = n_distinct(trip_date),
    total_trips     = sum(n_trips, na.rm = TRUE),
    mean_trips      = mean(n_trips, na.rm = TRUE),
    median_trips    = median(n_trips, na.rm = TRUE),
    min_trips       = min(n_trips, na.rm = TRUE),
    max_trips       = max(n_trips, na.rm = TRUE),
    sd_trips        = sd(n_trips, na.rm = TRUE)
  )
```
```{r}
summary_table
```
# Check for Missing Data
```{r}

trips <- trips %>%
  mutate(trip_date = as.Date(trip_date))
```
```{r}
trips <- trips %>%
  filter(!is.na(trip_date))
```

```{r}
full_dates <- tibble(
  trip_date = seq(min(trips$trip_date), max(trips$trip_date), by = "day")
)
```
```{r}
df_complete <- full_dates %>%
  left_join(trips, by = "trip_date")
```
```{r}
missing_dates <- df_complete %>%
  filter(is.na(n_trips))

missing_dates
n_missing <- nrow(missing_dates)
n_missing
```
```{r}
library(ggplot2)

ggplot(df_complete, aes(x = trip_date, y = is.na(n_trips))) +
  geom_point(alpha = 0.6) +
  scale_y_continuous(breaks = c(0,1), labels = c("Present", "Missing")) +
  labs(title = "Missing Daily BikeShare Data", x = "Date", y = "")
```
To identify missing days, we created a complete calendar sequence from July 2016 to October 2024 and merged it with our aggregated dataset. We found 218 missing days, primarily concentrated in 2016-01-13 to 2017-06-12. Based on documentation and visual inspection, these days correspond to reporting gaps in TTC's system or during the aggregation of the files where different date formats caused missing dates.
```{r}
library(forecast)
library(tseries)
library(ggplot2)
library(dplyr)
```

# Modelling
```{r}
# Sort just in case
trips <- trips[order(trips$trip_date), ]

train <- trips %>%
  filter(trip_date <= as.Date("2023-12-31"))

test <- trips %>%
  filter(trip_date >= as.Date("2024-01-01"))

y_train <- ts(train$n_trips, frequency = 7)
y_test  <- test$n_trips
```

# Preparing the data
```{r}
lambda <- BoxCox.lambda(y_train)
lambda
```
```{r}
y_train_bc <- BoxCox(y_train, lambda = lambda)
autoplot(y_train_bc) + ggtitle("Box-Cox Transformed Training Data")
```
The daily BikeShare series exhibits both weekly and annual seasonality. Since classical SARIMA models can only incorporate one seasonal period, we chose to model the weekly cycle (s = 7) in the Box–Jenkins framework, because day-of-week effects dominate short-term correlation. The longer annual cycle is more smoothly varying and is handled in our regression models rather than directly in the SARIMA structure.

```{r}
ndiffs(y_train_bc)    # suggested d
nsdiffs(y_train_bc)   # suggested D, with period 7
```
The algorithm does not think a seasonal difference at lag 7 is needed.
```{r}
d <- ndiffs(y_train_bc)

y_diff <- diff(y_train_bc,differences = d)
autoplot(y_diff) + ggtitle("Differenced (d) Series")
```
```{r}
ggtsdisplay(y_diff, main = "ACF & PACF After Differencing")
```
The weak repeating ACF at intervals of 7 supports a seasonal MA structure while the slowly decaying short-lag PACF means a pure ARIMA(p,1,q) with a small p/q is not fully resolving short-term dependence.
So we can conclude non-seasonal differencing alone leaves seasonal autocorrelation.
```{r}
y_diff2 <- diff(diff(y_train_bc, differences = 1), lag = 7, differences = 1)

ggtsdisplay(y_diff2, main = "ACF/PACF — With Seasonal Differencing (Lag 7)")
```
This pattern we see in the ACF and PACF is textbook SARIMA with seasonal MA(1) because their is a strong negative spike at the seasonal lag of 7 for MA(1) but no sharp seasonal cutoff, so we can exclude seasonal AR.

So the seasonal order should be (0,1,1)_7 and since the ACF and PACF both show nagative MA-like behaviour at lags 1-2, our candidate q can be q = 1 or 2 and p will probably be small like 0 or 1.
So we can try:
- ARIMA(0,1,1)
- ARIMA(1,1,1)
- ARIMA(0,1,2)
with a seasonal MA(1)
We will also try auto.arima() for comparison.

# Fit the models
```{r}
# Fit your 3 justified models
fit1 <- Arima(y_train_bc,
              order=c(0,1,1),
              seasonal=list(order=c(0,1,1), period=7),
              method="ML")

fit2 <- Arima(y_train_bc,
              order=c(1,1,1),
              seasonal=list(order=c(0,1,1), period=7),
              method="ML")

fit3 <- Arima(y_train_bc,
              order=c(0,1,2),
              seasonal=list(order=c(0,1,1), period=7),
              method="ML")

fit_auto <- auto.arima(y_train_bc, seasonal=TRUE, stepwise=FALSE, approximation=FALSE)
```
# Residual Diagnostics
```{r}
library(lawstat)
diagnose_model <- function(model, model_name = "Model") {
  
  cat("\n==============================\n")
  cat(" Diagnostics for:", model_name, "\n")
  cat("==============================\n\n")
  
  # Extract residuals
  res <- residuals(model)
  
  #### 1. AIC / BIC ####
  cat("AIC :", AIC(model), "\n")
  cat("BIC :", BIC(model), "\n\n")
  
  #### 2. In-sample RMSE ####
  rmse_in <- sqrt(mean(res^2, na.rm = TRUE))
  cat("In-sample RMSE:", rmse_in, "\n\n")
  
  #### 3. Q-Q Plot of residuals ####
  qqnorm(res, main = paste("Q-Q Plot:", model_name))
  qqline(res, col = "red")
  
  #### 4. Runs test (Independence) ####
  cat("\nRuns test for independence:\n")
  print(runs.test(res))
  
  #### 6. Plot residuals + ACF ####
  cat("\nResidual Diagnostics Plot:\n")
  print(checkresiduals(model))
  
  cat("\n\n")
  
  # Return RMSE_in, AIC, BIC for building a comparison table
  return(data.frame(
    Model = model_name,
    AIC = AIC(model),
    BIC = BIC(model),
    RMSE_in = rmse_in
  ))
}
```
```{r}
diag1 <- diagnose_model(fit1, "SARIMA(0,1,1)(0,1,1)[7]")
diag2 <- diagnose_model(fit2, "SARIMA(1,1,1)(0,1,1)[7]")
diag3 <- diagnose_model(fit3, "SARIMA(0,1,2)(0,1,1)[7]")
diag_auto <- diagnose_model(fit_auto, "auto.arima()")
```
```{r}
comparison_table <- bind_rows(diag1, diag2, diag3, diag_auto)
comparison_table
```
Box–Jenkins Model Selection

We examined several SARIMA models motivated by the ACF and PACF of the Box–Cox–transformed and differenced series. Based on the identification step, we fit three candidate models:
SARIMA(0,1,1)(0,1,1)[7]
SARIMA(1,1,1)(0,1,1)[7]
SARIMA(0,1,2)(0,1,1)[7]
as well as the model selected by auto.arima().

Each model was evaluated using:
AIC and BIC
In-sample RMSE
Q–Q plot for normality
Runs test for independence
Ljung–Box test for residual autocorrelation

The model SARIMA(0,1,1)(0,1,1)[7] performed poorly, failing both the runs test and the Ljung–Box test.
The model SARIMA(1,1,1)(0,1,1)[7] showed improved diagnostics but still exhibited evidence of residual dependence.
The auto.arima() model, ARIMA(1,1,1)(1,0,2)[7], achieved:
Lowest AIC: 13042.75
Lowest BIC: 13077.82
Lowest in-sample RMSE: 3.096

It also passed the Ljung–Box test (p = 0.149), indicating no remaining autocorrelation in the residuals. Although the runs test showed a mild deviation from randomness (p = 0.029), this is common for high-frequency time series and is acceptable given the strong performance in other diagnostics.

Based on these criteria, ARIMA(1,1,1)(1,0,2)[7] was selected as the final Box–Jenkins model.

# Forecast Test Set
```{r}
n_plot <- 30

h <- length(y_test)

fc1 <- forecast(fit1, h=h)
fc2 <- forecast(fit2, h=h)
fc3 <- forecast(fit3, h=h)
fc_auto <- forecast(fit_auto, h=h)

# Back-transform
fc1_back <- InvBoxCox(fc1$mean, lambda)
fc2_back <- InvBoxCox(fc2$mean, lambda)
fc3_back <- InvBoxCox(fc3$mean, lambda)
fc_auto_back <- InvBoxCox(fc_auto$mean, lambda)
fc_lower <- InvBoxCox(fc_auto$lower[,"95%"], lambda)
fc_upper <- InvBoxCox(fc_auto$upper[,"95%"], lambda)

# Compute accuracy
acc1 <- accuracy(fc1_back, y_test)
acc2 <- accuracy(fc2_back, y_test)
acc3 <- accuracy(fc3_back, y_test)
acc_auto <- accuracy(fc_auto_back, y_test)

acc1; acc2; acc3; acc_auto
```
```{r}
df_plot <- data.frame(
  date     = as.Date(test$trip_date[1:n_plot]),
  actual   = y_test[1:n_plot],
  forecast = as.numeric(fc_auto_back[1:n_plot]),
  lower95  = as.numeric(fc_lower[1:n_plot]),
  upper95  = as.numeric(fc_upper[1:n_plot])
)
```
```{r}
ggplot(df_plot, aes(x = date)) +
  # 95% prediction interval band
  geom_ribbon(aes(ymin = lower95, ymax = upper95),
              fill = "lightblue", alpha = 0.4) +
  # Forecast line
  geom_line(aes(y = forecast), color = "blue", size = 1.2) +
  # Actual values
  geom_line(aes(y = actual), color = "black", size = 1.2) +
  labs(
    title = "First 30 Days of Test Set: Actual vs Forecast (95% PI)",
    x = "Date",
    y = "Daily Trips"
  ) +
  theme_minimal() +
  theme(
    text = element_text(size = 12),
    plot.title = element_text(face = "bold", size = 14)
  )
```
```{r}
final_reg <- readRDS("final_reg.rds")
resid_reg <- residuals(final_reg)
```
```{r}

# convert to ts with weekly frequency
resid_ts <- ts(resid_reg, frequency = 7)

library(forecast)
ggtsdisplay(resid_ts, main = "ACF & PACF of Regression Residuals")

```

```{r}
resid_diff1 <- diff(resid_ts, differences = 1)
ggtsdisplay(resid_diff1)
```
```{r}
resid_diff2 <- diff(resid_diff1, lag = 7, differences = 1)
ggtsdisplay(resid_diff2)
```
ACF after d = 1, D = 1 shows a large negative spike at lag 1 and at lag 7, with other lags inside the confidence bands, suggesting both a non-seasonal MA(1) and a seasonal MA(1) component. The PACF displays a gradually decaying pattern across lags 1–4 and a damped seasonal pattern at multiples of 7, which is consistent with an MA-type model rather than AR. Therefore, we considered SARIMA(1,1,1)(0,1,1)[7] as our primary candidate model for the regression residuals.
```{r}
fit_sarima_resid <- Arima(
  resid_ts,
  order = c(1,1,1),
  seasonal = list(order = c(0,1,1), period = 7),
  method = "ML"
)

summary(fit_sarima_resid)
checkresiduals(fit_sarima_resid)
```




