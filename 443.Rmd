Summary Table of Data
```{r}
# Summary of Data
trips <- read.csv("trips_per_day.csv")
```


```{r}
library(dplyr)
library(lubridate)

trips <- trips %>%
  mutate(trip_date = as.Date(trip_date))

summary_table <- trips %>%
  summarise(
    time_span_start = min(trip_date, na.rm = TRUE),
    time_span_end   = max(trip_date, na.rm = TRUE),
    total_days      = n_distinct(trip_date),
    total_trips     = sum(n_trips, na.rm = TRUE),
    mean_trips      = mean(n_trips, na.rm = TRUE),
    median_trips    = median(n_trips, na.rm = TRUE),
    min_trips       = min(n_trips, na.rm = TRUE),
    max_trips       = max(n_trips, na.rm = TRUE),
    sd_trips        = sd(n_trips, na.rm = TRUE)
  )
```
```{r}
summary_table
```
# Check for Missing Data
```{r}

trips <- trips %>%
  mutate(trip_date = as.Date(trip_date))
```
```{r}
trips <- trips %>%
  filter(!is.na(trip_date))
```

```{r}
full_dates <- tibble(
  trip_date = seq(min(trips$trip_date), max(trips$trip_date), by = "day")
)
```
```{r}
df_complete <- full_dates %>%
  left_join(trips, by = "trip_date")
```
```{r}
missing_dates <- df_complete %>%
  filter(is.na(n_trips))

missing_dates
n_missing <- nrow(missing_dates)
n_missing
```
```{r}
library(ggplot2)

ggplot(df_complete, aes(x = trip_date, y = is.na(n_trips))) +
  geom_point(alpha = 0.6) +
  scale_y_continuous(breaks = c(0,1), labels = c("Present", "Missing")) +
  labs(title = "Missing Daily BikeShare Data", x = "Date", y = "")
```
To identify missing days, we created a complete calendar sequence from July 2016 to October 2024 and merged it with our aggregated dataset. We found 218 missing days, primarily concentrated in 2016-01-13 to 2017-06-12. Based on documentation and visual inspection, these days correspond to reporting gaps in TTC's system or during the aggregation of the files where different date formats caused missing dates.
```{r}
library(forecast)
library(tseries)
library(ggplot2)
library(dplyr)
```

# Modelling
```{r}
# Sort just in case
trips <- trips[order(trips$trip_date), ]

train <- trips %>%
  filter(trip_date <= as.Date("2023-12-31"))

test <- trips %>%
  filter(trip_date >= as.Date("2024-01-01"))

y_train <- ts(train$n_trips, frequency = 7)
y_test  <- test$n_trips
```

# Preparing the data
```{r}
lambda <- BoxCox.lambda(y_train)
lambda
```
```{r}
y_train_bc <- BoxCox(y_train, lambda = lambda)
autoplot(y_train_bc) + ggtitle("Box-Cox Transformed Training Data")
```
The daily BikeShare series exhibits both weekly and annual seasonality. Since classical SARIMA models can only incorporate one seasonal period, we chose to model the weekly cycle (s = 7) in the Box–Jenkins framework, because day-of-week effects dominate short-term correlation. The longer annual cycle is more smoothly varying and is handled in our regression models rather than directly in the SARIMA structure.

```{r}
ndiffs(y_train_bc)    # suggested d
nsdiffs(y_train_bc)   # suggested D, with period 7
```
The algorithm does not think a seasonal difference at lag 7 is needed.
```{r}
d <- ndiffs(y_train_bc)

y_diff <- diff(y_train_bc,differences = d)
autoplot(y_diff) + ggtitle("Differenced (d) Series")
```
```{r}
ggtsdisplay(y_diff, main = "ACF & PACF After Differencing")
```
The weak repeating ACF at intervals of 7 supports a seasonal MA structure while the slowly decaying short-lag PACF means a pure ARIMA(p,1,q) with a small p/q is not fully resolving short-term dependence.
So we can conclude non-seasonal differencing alone leaves seasonal autocorrelation.
```{r}
y_diff2 <- diff(diff(y_train_bc, differences = 1), lag = 7, differences = 1)

ggtsdisplay(y_diff2, main = "ACF/PACF — With Seasonal Differencing (Lag 7)")
```
This pattern we see in the ACF and PACF is textbook SARIMA with seasonal MA(1) because their is a strong negative spike at the seasonal lag of 7 for MA(1) but no sharp seasonal cutoff, so we can exclude seasonal AR.

So the seasonal order should be (0,1,1)_7 and since the ACF and PACF both show nagative MA-like behaviour at lags 1-2, our candidate q can be q = 1 or 2 and p will probably be small like 0 or 1.
So we can try:
- ARIMA(0,1,1)
- ARIMA(1,1,1)
- ARIMA(0,1,2)
with a seasonal MA(1)
We will also try auto.arima() for comparison.

# Fit the models
```{r}
# Fit your 3 justified models
fit1 <- Arima(y_train_bc,
              order=c(0,1,1),
              seasonal=list(order=c(0,1,1), period=7),
              method="ML")

fit2 <- Arima(y_train_bc,
              order=c(1,1,1),
              seasonal=list(order=c(0,1,1), period=7),
              method="ML")

fit3 <- Arima(y_train_bc,
              order=c(0,1,2),
              seasonal=list(order=c(0,1,1), period=7),
              method="ML")

fit_auto <- auto.arima(y_train_bc, seasonal=TRUE, stepwise=FALSE, approximation=FALSE)
```
# Residual Diagnostics
```{r}
library(lawstat)
diagnose_model <- function(model, model_name = "Model") {
  
  cat("\n==============================\n")
  cat(" Diagnostics for:", model_name, "\n")
  cat("==============================\n\n")
  
  # Extract residuals
  res <- residuals(model)
  
  #### 1. AIC / BIC ####
  cat("AIC :", AIC(model), "\n")
  cat("BIC :", BIC(model), "\n\n")
  
  #### 2. In-sample RMSE ####
  rmse_in <- sqrt(mean(res^2, na.rm = TRUE))
  cat("In-sample RMSE:", rmse_in, "\n\n")
  
  #### 3. Q-Q Plot of residuals ####
  qqnorm(res, main = paste("Q-Q Plot:", model_name))
  qqline(res, col = "red")
  
  #### 4. Runs test (Independence) ####
  cat("\nRuns test for independence:\n")
  print(runs.test(res))
  
  #### 6. Plot residuals + ACF ####
  cat("\nResidual Diagnostics Plot:\n")
  print(checkresiduals(model))
  
  cat("\n\n")
  
  # Return RMSE_in, AIC, BIC for building a comparison table
  return(data.frame(
    Model = model_name,
    AIC = AIC(model),
    BIC = BIC(model),
    RMSE_in = rmse_in
  ))
}
```
```{r}
diag1 <- diagnose_model(fit1, "SARIMA(0,1,1)(0,1,1)[7]")
diag2 <- diagnose_model(fit2, "SARIMA(1,1,1)(0,1,1)[7]")
diag3 <- diagnose_model(fit3, "SARIMA(0,1,2)(0,1,1)[7]")
diag_auto <- diagnose_model(fit_auto, "auto.arima()")
```
```{r}
comparison_table <- bind_rows(diag1, diag2, diag3, diag_auto)
comparison_table
```
Box–Jenkins Model Selection

We examined several SARIMA models motivated by the ACF and PACF of the Box–Cox–transformed and differenced series. Based on the identification step, we fit three candidate models:
SARIMA(0,1,1)(0,1,1)[7]
SARIMA(1,1,1)(0,1,1)[7]
SARIMA(0,1,2)(0,1,1)[7]
as well as the model selected by auto.arima().

Each model was evaluated using:
AIC and BIC
In-sample RMSE
Q–Q plot for normality
Runs test for independence
Ljung–Box test for residual autocorrelation

The model SARIMA(0,1,1)(0,1,1)[7] performed poorly, failing both the runs test and the Ljung–Box test.
The model SARIMA(1,1,1)(0,1,1)[7] showed improved diagnostics but still exhibited evidence of residual dependence.
The auto.arima() model, ARIMA(1,1,1)(1,0,2)[7], achieved:
Lowest AIC: 13042.75
Lowest BIC: 13077.82
Lowest in-sample RMSE: 3.096

It also passed the Ljung–Box test (p = 0.149), indicating no remaining autocorrelation in the residuals. Although the runs test showed a mild deviation from randomness (p = 0.029), this is common for high-frequency time series and is acceptable given the strong performance in other diagnostics.

Based on these criteria, ARIMA(1,1,1)(1,0,2)[7] was selected as the final Box–Jenkins model.

# Forecast Test Set
```{r}
n_plot <- 30

h <- length(y_test)

fc1 <- forecast(fit1, h=h)
fc2 <- forecast(fit2, h=h)
fc3 <- forecast(fit3, h=h)
fc_auto <- forecast(fit_auto, h=h)

# Back-transform
fc1_back <- InvBoxCox(fc1$mean, lambda)
fc2_back <- InvBoxCox(fc2$mean, lambda)
fc3_back <- InvBoxCox(fc3$mean, lambda)
fc_auto_back <- InvBoxCox(fc_auto$mean, lambda)
fc_lower <- InvBoxCox(fc_auto$lower[,"95%"], lambda)
fc_upper <- InvBoxCox(fc_auto$upper[,"95%"], lambda)

# Compute accuracy
acc1 <- accuracy(fc1_back, y_test)
acc2 <- accuracy(fc2_back, y_test)
acc3 <- accuracy(fc3_back, y_test)
acc_auto <- accuracy(fc_auto_back, y_test)

acc1; acc2; acc3; acc_auto
```
```{r}
df_plot <- data.frame(
  date     = as.Date(test$trip_date[1:n_plot]),
  actual   = y_test[1:n_plot],
  forecast = as.numeric(fc_auto_back[1:n_plot]),
  lower95  = as.numeric(fc_lower[1:n_plot]),
  upper95  = as.numeric(fc_upper[1:n_plot])
)
```
```{r}
ggplot(df_plot, aes(x = date)) +
  # 95% prediction interval band
  geom_ribbon(aes(ymin = lower95, ymax = upper95),
              fill = "lightblue", alpha = 0.4) +
  # Forecast line
  geom_line(aes(y = forecast), color = "blue", size = 1.2) +
  # Actual values
  geom_line(aes(y = actual), color = "black", size = 1.2) +
  labs(
    title = "First 30 Days of Test Set: Actual vs Forecast (95% PI)",
    x = "Date",
    y = "Daily Trips"
  ) +
  theme_minimal() +
  theme(
    text = element_text(size = 12),
    plot.title = element_text(face = "bold", size = 14)
  )
```
```{r}
final_reg <- readRDS("final_reg.rds")
resid_reg <- residuals(final_reg)
```
```{r}

# convert to ts with weekly frequency
resid_ts <- ts(resid_reg, frequency = 7)

library(forecast)
library(ggplot2)
library(patchwork)
p_ts <- autoplot(resid_ts) +
  ggtitle("Residuals")

p_acf <- ggAcf(resid_ts) +
  ggtitle("ACF")

p_pacf <- ggPacf(resid_ts) +
  ggtitle("PACF")

(p_ts / (p_acf | p_pacf)) +
  plot_annotation(title = "ACF & PACF of Regression Residuals")
```
```{r}
# Determine Order of Seasonal and Non-Seasonal Lags
trend_lag <- ndiffs(resid_ts)
seasonal_lag <- ndiffs(resid_ts,m=7)
trend_lag ; seasonal_lag
```

```{r}
resid_diff1 <- diff(resid_ts, differences = trend_lag)
ggtsdisplay(resid_diff1)
```
```{r}
resid_diff2 <- diff(resid_diff1, lag = 7, differences = seasonal_lag)
ts_diff_plot   <- autoplot(resid_diff2) +
  ggtitle("Seasonally Differenced Residuals")

acf_diff_plot  <- ggAcf(resid_diff2) +
  ggtitle("ACF")

pacf_diff_plot <- ggPacf(resid_diff2) +
  ggtitle("PACF")

# Combine into a layout similar to ggtsdisplay()
(ts_diff_plot / (acf_diff_plot | pacf_diff_plot)) +
  plot_annotation(title = "ACF & PACF of Seasonal & Non-Seasonal Differenced Residuals")
```
After differencing d=1 and D=1, we get our stationary version of the residuals.
Looking at the non-seasonal part, we see from the ACF there is a strong spike at lag 1, then basically noise. For the PACF, it's gradually decaying at short lags. This is MA(1) behavior but also could be AR(1) behavior because I think the lag 1 of the PACF might be strong so p = {0,1} and q = 1.

As for the seasonal parts, in the ACF, I see a big lag at 7 and the other seasonal lags are within the confidence bands.For the PACF, we see at spike at Lag 7 that decays seasonally. So will pick P = 0 and Q = 1.
```{r}
model1 <- Arima(resid_ts,
              order=c(0,1,1),
              seasonal=list(order=c(0,1,1), period=7),
              method="ML")

model2 <- Arima(resid_ts,
              order=c(1,1,1),
              seasonal=list(order=c(0,1,1), period=7),
              method="ML")
model_auto <- auto.arima(resid_ts, seasonal=TRUE, stepwise=FALSE, approximation=FALSE)
```
```{r}
diagnostics1 <- diagnose_model(model1, "SARIMA(0,1,1)(0,1,1)[7]")
diagnostics2 <- diagnose_model(model2, "SARIMA(1,1,1)(0,1,1)[7]")
diagnosticsauto <- diagnose_model(model_auto, "auto.arima()")
```
We fit several SARIMA models to the regression residuals, including SARIMA(0,1,1)(0,1,1)_7, SARIMA(1,1,1)(0,1,1)_7, and the model selected by auto.arima(), ARIMA(1,1,1)(1,0,1)_7. The SARIMA(0,1,1)(0,1,1)_7 model performed poorly, with very low p-values in the Ljung–Box and runs tests, indicating substantial residual autocorrelation and non-randomness.

Both SARIMA(1,1,1)(0,1,1)_7 and ARIMA(1,1,1)(1,0,1)_7 showed similar mostly-stationary residual ACF/PACF patterns. The auto.arima model achieved slightly lower AIC/BIC and in-sample RMSE, but its residuals exhibited stronger remaining autocorrelation (Ljung–Box p ≈ 0.002). In contrast, SARIMA(1,1,1)(0,1,1)_7 provided a good compromise between fit and parsimony, with substantially improved diagnostics (runs test p ≈ 0.097 and Ljung–Box p ≈ 0.014).

Given the large sample size, the Ljung–Box test is highly sensitive to very small deviations from white noise, so we prioritized a simpler model with cleaner residual structure over a marginal reduction in AIC. We therefore selected SARIMA(1,1,1)(0,1,1)_7 as the final Box–Jenkins model for the regression residuals.

```{r}
# Load RDS Files
final_train_data <- readRDS("final_train_data.rds")
final_test_data <- readRDS("final_test_data.rds")
poly_basis_final <- readRDS("poly_basis_final.rds")
test_data <- readRDS("test_data_processed.rds")
lam <- readRDS("boxcox_lambda.rds")
```
```{r}
reg_forecast_trans <- predict(final_reg, newdata = final_test_data)
```
```{r}
h <- nrow(final_test_data)

sarima_fc <- forecast(model2, h = h)
sarima_resid_forecast <- sarima_fc$mean
```
```{r}
hybrid_forecast_trans <- reg_forecast_trans + sarima_resid_forecast
```
```{r}
hybrid_forecast <- InvBoxCox(hybrid_forecast_trans, lambda = lam)
actual_2024 <- InvBoxCox(final_test_data$y, lambda = lam)
```

```{r}
accuracy_hybrid <- accuracy(hybrid_forecast, actual_2024)
```
```{r}
plot(test_data$trip_date, actual_2024, type = "l", col = "gray", lwd = 2,
     main = "2024 Forecast vs Actuals (OLS)", ylab = "Transformed Trips", xlab = "Date")
lines(test_data$trip_date, hybrid_forecast, col = "red", lwd = 2)
legend("topright", legend = c("Actual 2024 Data", "Hybrid Forecast"), 
       col = c("gray", "red"), lty = 1, lwd = 2)
```
```{r}
apse_test <- mean((actual_2024 - hybrid_forecast)^2)
```
```{r}
accuracy_hybrid
```
```{r}
library(knitr)
accuracy_hybrid_df <- as.data.frame(accuracy_hybrid)

accuracy_hybrid_df$APSE <- apse_test

print(accuracy_hybrid_df, row.names = FALSE)
```

Interpreting each accuracy number:
1. ME
The combined mode on average over-forecasts the true daily trips by about 7900 trips per day which indicates large bias and systematic overprediction.
2. RMSE
My forecasts are off by about 10,100 trips on a typical data but since the daily demand ranges from 3000 to 40000, this magnitude may or may not be large depending on seasonaly volatility
3. MAE
As for the Average absolute error in trips, the model is off by 8200 trips per day.
4. MPE
This model over predicts by 32% on average
5. MAPE
A typical day's forecast is about 38% off.

This means our combined regression + SARIMA residual model is strongly biased, with high error relative to the actual values which means it's not capturing 2024 behavior well and it doesn't provide any meaningful improvement over the regression-only forecasting.

So why is this error so high?
1. Something in 2024 changed structurally, so it's not generalizing well to 2024.
2. The regression model might be overfitting seasonal factors and even though we did CV, the categorical seasonality may not extrapolate well to 2024.
3. Even for the sarima model we picked, it had a Ljung-Box p-value close to 0.01 which is still non-white noise.
4. Also since we calculated accuracy on the Box-Cox inverse transformation, it may be producing upward bias since it can inflate predictions when residual variance is large. Also the inverse Box-Cox is biased when forecasting which is causing this issue of overforecasting.

I think we should add weather data since BikeShare data is extremely weather-driven and omitting the temperature, rainy days or snowy days will inflate the forecast error. 

